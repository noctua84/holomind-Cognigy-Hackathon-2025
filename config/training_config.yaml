# Training hyperparameters and settings
training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  
  optimizer:
    type: "adam"
    betas: [0.9, 0.999]
    weight_decay: 0.0001
    
  scheduler:
    type: "cosine"
    warmup_epochs: 5
    min_lr: 0.00001
    
  early_stopping:
    patience: 10
    min_delta: 0.001
    
  checkpointing:
    save_frequency: 10  # epochs
    max_checkpoints: 5
    save_best: true
    
  validation:
    frequency: 1  # epochs
    metrics: ["loss", "accuracy", "forgetting"] 